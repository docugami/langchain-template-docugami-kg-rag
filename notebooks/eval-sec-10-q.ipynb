{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEC 10-Q Eval\n",
    "\n",
    "Evaluating Docugami KG-RAG against OpenAI Assistants Retrieval for this dataset: https://github.com/docugami/KG-RAG-datasets/tree/main/sec-10-q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent pid 15351\n",
      "Identity added: /root/.ssh/id_rsa (/root/.ssh/id_rsa)\n",
      "dev environment initialized\n",
      "Agent pid 15361\n",
      "Identity added: /root/.ssh/id_rsa (/root/.ssh/id_rsa)\n",
      "dev environment initialized\n",
      "Cloning into 'temp'...\n",
      "remote: Enumerating objects: 197, done.\u001b[K\n",
      "remote: Counting objects: 100% (90/90), done.\u001b[K\n",
      "remote: Compressing objects: 100% (71/71), done.\u001b[K\n",
      "remote: Total 197 (delta 20), reused 67 (delta 10), pack-reused 107\u001b[K\n",
      "Receiving objects: 100% (197/197), 56.53 MiB | 27.27 MiB/s, done.\n",
      "Resolving deltas: 100% (21/21), done.\n"
     ]
    }
   ],
   "source": [
    "!rm -rf temp\n",
    "!git clone https://github.com/docugami/KG-RAG-datasets.git temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Important: Create your OpenAI assistant via https://platform.openai.com/playground\n",
    "#            and put the assistant ID here. Make sure you upload the identical set of\n",
    "#            files listed below (these files will be uploaded automatically to Docugami)\n",
    "OPENAI_ASSISTANT_ID = \"asst_qY1M0SeFYlmqkEZsMVZX2VAK\"\n",
    "\n",
    "DOCSET_NAME = \"SEC 10Q Filings\"\n",
    "EVAL_NAME = DOCSET_NAME + \" \" + datetime.now().strftime(\"%Y-%m-%d\")\n",
    "FILES_DIR = Path(os.getcwd()) / \"temp/sec-10-q/data/v1/docs\"\n",
    "FILE_NAMES = [\n",
    "    \"2022 Q3 AAPL.pdf\",\n",
    "    \"2022 Q3 AMZN.pdf\",\n",
    "    \"2022 Q3 INTC.pdf\",\n",
    "    \"2022 Q3 MSFT.pdf\",\n",
    "    \"2022 Q3 NVDA.pdf\",\n",
    "    \"2023 Q1 AAPL.pdf\",\n",
    "    \"2023 Q1 AMZN.pdf\",\n",
    "    \"2023 Q1 INTC.pdf\",\n",
    "    \"2023 Q1 MSFT.pdf\",\n",
    "    \"2023 Q1 NVDA.pdf\",\n",
    "    \"2023 Q2 AAPL.pdf\",\n",
    "    \"2023 Q2 AMZN.pdf\",\n",
    "    \"2023 Q2 INTC.pdf\",\n",
    "    \"2023 Q2 MSFT.pdf\",\n",
    "    \"2023 Q2 NVDA.pdf\",\n",
    "    \"2023 Q3 AAPL.pdf\",\n",
    "    \"2023 Q3 AMZN.pdf\",\n",
    "    \"2023 Q3 INTC.pdf\",\n",
    "    \"2023 Q3 MSFT.pdf\",\n",
    "    \"2023 Q3 NVDA.pdf\",\n",
    "]\n",
    "\n",
    "# Using mini set to save cost while developing, use full set for actual runs (~$300 per run in OpenAI costs per run)\n",
    "GROUND_TRUTH_CSV = Path(os.getcwd()) / \"temp/sec-10-q/data/v1/qna_data_mini.csv\"\n",
    "\n",
    "# We will run each experiment multiple times and average,\n",
    "# since results vary slightly over runs\n",
    "PER_EXPERIMENT_RUN_COUNT = 5\n",
    "\n",
    "# Note: Please specify ~6 (or more!) similar files to process together as a document set\n",
    "# This is currently a requirement for Docugami to automatically detect motifs\n",
    "# across the document set to generate a semantic XML Knowledge Graph.\n",
    "assert len(FILE_NAMES) >= 6, \"Please provide at least 6 files\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langsmith import Client\n",
    "\n",
    "# Read\n",
    "df = pd.read_csv(GROUND_TRUTH_CSV)\n",
    "\n",
    "# Dataset\n",
    "client = Client()\n",
    "dataset_name = EVAL_NAME\n",
    "existing_datasets = list(client.list_datasets(dataset_name=dataset_name))\n",
    "if existing_datasets:\n",
    "    # read existing dataset\n",
    "    dataset = client.read_dataset(dataset_name=dataset_name)\n",
    "else:\n",
    "    dataset = client.create_dataset(dataset_name=dataset_name)\n",
    "    # Populate dataset\n",
    "    for _, row in df.iterrows():\n",
    "        q = row[\"Question\"]\n",
    "        a = row[\"Answer\"]\n",
    "        client.create_example(\n",
    "            inputs={\"question\": q}, outputs={\"answer\": a}, dataset_id=dataset.id\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Docugami KG-RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload files to Docugami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docugami import Docugami\n",
    "from docugami.lib.upload import upload_to_named_docset, wait_for_dgml\n",
    "\n",
    "dg_client = Docugami()\n",
    "file_paths = [FILES_DIR / file_name for file_name in FILE_NAMES]\n",
    "\n",
    "# Files will not be re-uploaded if they were previously uploaded (based on name)\n",
    "dg_docs = upload_to_named_docset(dg_client, file_paths, DOCSET_NAME)\n",
    "\n",
    "docset_id = \"\"\n",
    "docset_name = \"\"\n",
    "for doc in dg_docs:\n",
    "    if not docset_id:\n",
    "        docset_id = doc.docset.id\n",
    "    else:\n",
    "        # all docs must be in the same docset\n",
    "        assert docset_id == doc.docset.id\n",
    "\n",
    "    if not docset_name:\n",
    "        docset_name = dg_client.docsets.retrieve(doc.docset.id).name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'2022 Q3 AAPL.pdf': '/tmp/tmpxkmsgl_r',\n",
       " '2022 Q3 AMZN.pdf': '/tmp/tmpymuy8le2',\n",
       " '2022 Q3 INTC.pdf': '/tmp/tmpapatuozy',\n",
       " '2022 Q3 MSFT.pdf': '/tmp/tmpysegu_5a',\n",
       " '2022 Q3 NVDA.pdf': '/tmp/tmpahcqyed8',\n",
       " '2023 Q1 AAPL.pdf': '/tmp/tmpwx2xlilf',\n",
       " '2023 Q1 AMZN.pdf': '/tmp/tmpukrubb11',\n",
       " '2023 Q1 INTC.pdf': '/tmp/tmpx7yqwynq',\n",
       " '2023 Q1 MSFT.pdf': '/tmp/tmpwmjki_gc',\n",
       " '2023 Q1 NVDA.pdf': '/tmp/tmp3i4rl0zm',\n",
       " '2023 Q2 AAPL.pdf': '/tmp/tmpwbgcvrat',\n",
       " '2023 Q2 AMZN.pdf': '/tmp/tmpt8f7olid',\n",
       " '2023 Q2 INTC.pdf': '/tmp/tmpg47117r2',\n",
       " '2023 Q2 MSFT.pdf': '/tmp/tmpzlxyeqif',\n",
       " '2023 Q2 NVDA.pdf': '/tmp/tmp_j91xb6q',\n",
       " '2023 Q3 AAPL.pdf': '/tmp/tmpg0lyyvyh',\n",
       " '2023 Q3 AMZN.pdf': '/tmp/tmp3pys3zri',\n",
       " '2023 Q3 INTC.pdf': '/tmp/tmpis26tk2w',\n",
       " '2023 Q3 MSFT.pdf': '/tmp/tmp38jmza21',\n",
       " '2023 Q3 NVDA.pdf': '/tmp/tmpb64158l6'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Wait for files to finish processing (OCR, and zero-shot creation of XML knowledge graph)\n",
    "\n",
    "# Note: This can take some time on the free docugami tier (up to ~20 mins). Please contact us for faster paid plans.\n",
    "wait_for_dgml(dg_client, dg_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch not installed...\n",
      "Loading default rankgpt3 model for language en\n",
      "Loading RankGPTRanker model gpt-3.5-turbo\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for ReActAgent\nstandalone_question_chain\n  field required (type=value_error.missing)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Run indexing\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdocugami_kg_rag\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindexing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m index_docset\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m docset_id\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m docset_name\n",
      "File \u001b[0;32m~/Source/github/langchain-template-docugami-kg-rag/docugami_kg_rag/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdocugami_kg_rag\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magent\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m agent\n\u001b[1;32m      3\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124magent\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/Source/github/langchain-template-docugami-kg-rag/docugami_kg_rag/agent.py:128\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m cited_answer\u001b[38;5;241m.\u001b[39manswer\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    123\u001b[0m agent \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    124\u001b[0m     {\n\u001b[1;32m    125\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m x: get_question_from_messages(x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m]),\n\u001b[1;32m    126\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchat_history\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m x: get_chat_history_from_messages(x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m]),\n\u001b[1;32m    127\u001b[0m     }\n\u001b[0;32m--> 128\u001b[0m     \u001b[38;5;241m|\u001b[39m \u001b[43mReActAgent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m        \u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLARGE_CONTEXT_INSTRUCT_LLM\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m        \u001b[49m\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEMBEDDINGS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuild_tools\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mrunnable()\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;241m|\u001b[39m RunnableLambda(agent_output_to_string)\n\u001b[1;32m    134\u001b[0m )\u001b[38;5;241m.\u001b[39mwith_types(\n\u001b[1;32m    135\u001b[0m     input_type\u001b[38;5;241m=\u001b[39mAgentInput,  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    136\u001b[0m )\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mgettrace():\n\u001b[1;32m    140\u001b[0m         \u001b[38;5;66;03m# This code will only run if a debugger is attached\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/docugami-kg-rag-sMPCFT4i-py3.9/lib/python3.9/site-packages/pydantic/v1/main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m validate_model(__pydantic_self__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, data)\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[0;32m--> 341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m'\u001b[39m, values)\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for ReActAgent\nstandalone_question_chain\n  field required (type=value_error.missing)"
     ]
    }
   ],
   "source": [
    "# Run indexing\n",
    "from docugami_kg_rag.indexing import index_docset\n",
    "\n",
    "assert docset_id\n",
    "assert docset_name\n",
    "\n",
    "# Note: This can take some time since it is embedding and creating summaries for all the docs and chunks\n",
    "index_docset(docset_id=docset_id, name=docset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Docugami Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docugami_kg_rag.agent import agent as docugami_agent\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "def predict_docugami_agent(question: str) -> str:\n",
    "    return docugami_agent.invoke(\n",
    "        {\n",
    "            \"messages\": [HumanMessage(content=question)],\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the agent to make sure it is working\n",
    "predict_docugami_agent(\"How much did Microsoft spend for opex in the latest quarter?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up OpenAI Assistants Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create OpenAI Agent\n",
    "\n",
    "Please go to https://platform.openai.com/playground and create your agent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.openai_assistant import OpenAIAssistantRunnable\n",
    "\n",
    "def predict_openai_agent(input: dict, config: dict = None) -> dict:\n",
    "    openai_agent = OpenAIAssistantRunnable(assistant_id=OPENAI_ASSISTANT_ID, as_agent=True).with_config(config)\n",
    "    question = input[\"question\"]\n",
    "    result = openai_agent.invoke({\"content\": question})\n",
    "\n",
    "    return result.return_values[\"output\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the agent to make sure it is working\n",
    "predict_openai_agent({\"question\": \"How much did Microsoft spend for opex in the latest quarter?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Evals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from langsmith.client import Client\n",
    "from langchain.smith import RunEvalConfig\n",
    "from langchain.globals import set_llm_cache, get_llm_cache\n",
    "\n",
    "eval_config = RunEvalConfig(\n",
    "    evaluators=[\"qa\"],\n",
    ")\n",
    "\n",
    "\n",
    "def run_eval(eval_func, eval_run_name):\n",
    "    \"\"\"\n",
    "    Run eval\n",
    "    \"\"\"\n",
    "    client = Client()\n",
    "    client.run_on_dataset(\n",
    "        dataset_name=EVAL_NAME,\n",
    "        llm_or_chain_factory=eval_func,\n",
    "        evaluation=eval_config,\n",
    "        verbose=True,\n",
    "        project_name=eval_run_name,\n",
    "        concurrency_level=2,  # Reduced to help with rate limits, but will take longer\n",
    "    )\n",
    "\n",
    "\n",
    "# Experiments\n",
    "agent_map = {\n",
    "    \"docugami_kg_rag_zero_shot\": predict_docugami_agent,\n",
    "    \"openai_assistant_retrieval\": predict_openai_agent,\n",
    "}\n",
    "\n",
    "try:\n",
    "    # Disable global cache setting to get fresh results every time for all experiments\n",
    "    # since no caching or temperature-0 is supported for the openai assistants API and\n",
    "    # we want to measure under similar conditions\n",
    "    cache = get_llm_cache()\n",
    "    set_llm_cache(None)\n",
    "\n",
    "    for i in range(PER_EXPERIMENT_RUN_COUNT):\n",
    "        run_id = str(uuid.uuid4())\n",
    "        for project_name, agent in agent_map.items():\n",
    "            run_eval(agent, project_name + \"_\" + run_id)\n",
    "finally:\n",
    "    # Revert cache setting to global default\n",
    "    set_llm_cache(cache)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "app-sMPCFT4i-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
