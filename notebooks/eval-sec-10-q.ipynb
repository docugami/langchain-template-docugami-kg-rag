{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEC 10-Q Eval\n",
    "\n",
    "Evaluating Docugami KG-RAG against OpenAI Assistants Retrieval for this dataset: https://github.com/docugami/KG-RAG-datasets/tree/main/sec-10-q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf temp\n",
    "!git clone https://github.com/docugami/KG-RAG-datasets.git temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Important: Create your OpenAI assistant via https://platform.openai.com/playground\n",
    "#            and put the assistant ID here. Make sure you upload the identical set of\n",
    "#            files listed below (these files will be uploaded automatically to Docugami)\n",
    "OPENAI_ASSISTANT_ID = \"asst_qY1M0SeFYlmqkEZsMVZX2VAK\"\n",
    "\n",
    "DOCSET_NAME = \"SEC 10Q Filings\"\n",
    "EVAL_NAME = DOCSET_NAME + \" \" + datetime.now().strftime(\"%Y-%m-%d\")\n",
    "FILES_DIR = Path(os.getcwd()) / \"temp/sec-10-q/data/v1/docs\"\n",
    "FILE_NAMES = [\n",
    "    \"2022 Q3 AAPL.pdf\",\n",
    "    \"2022 Q3 AMZN.pdf\",\n",
    "    \"2022 Q3 INTC.pdf\",\n",
    "    \"2022 Q3 MSFT.pdf\",\n",
    "    \"2022 Q3 NVDA.pdf\",\n",
    "    \"2023 Q1 AAPL.pdf\",\n",
    "    \"2023 Q1 AMZN.pdf\",\n",
    "    \"2023 Q1 INTC.pdf\",\n",
    "    \"2023 Q1 MSFT.pdf\",\n",
    "    \"2023 Q1 NVDA.pdf\",\n",
    "    \"2023 Q2 AAPL.pdf\",\n",
    "    \"2023 Q2 AMZN.pdf\",\n",
    "    \"2023 Q2 INTC.pdf\",\n",
    "    \"2023 Q2 MSFT.pdf\",\n",
    "    \"2023 Q2 NVDA.pdf\",\n",
    "    \"2023 Q3 AAPL.pdf\",\n",
    "    \"2023 Q3 AMZN.pdf\",\n",
    "    \"2023 Q3 INTC.pdf\",\n",
    "    \"2023 Q3 MSFT.pdf\",\n",
    "    \"2023 Q3 NVDA.pdf\",\n",
    "]\n",
    "\n",
    "# Using mini set to save cost while developing, use full set for actual runs (~$300 per run in OpenAI costs per run)\n",
    "GROUND_TRUTH_CSV = Path(os.getcwd()) / \"temp/sec-10-q/data/v1/qna_data_mini.csv\"\n",
    "\n",
    "# We will run each experiment multiple times and average,\n",
    "# since results vary slightly over runs\n",
    "PER_EXPERIMENT_RUN_COUNT = 5\n",
    "\n",
    "# Note: Please specify ~6 (or more!) similar files to process together as a document set\n",
    "# This is currently a requirement for Docugami to automatically detect motifs\n",
    "# across the document set to generate a semantic XML Knowledge Graph.\n",
    "assert len(FILE_NAMES) >= 6, \"Please provide at least 6 files\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langsmith import Client\n",
    "\n",
    "# Read\n",
    "df = pd.read_csv(GROUND_TRUTH_CSV)\n",
    "\n",
    "# Dataset\n",
    "client = Client()\n",
    "dataset_name = EVAL_NAME\n",
    "existing_datasets = list(client.list_datasets(dataset_name=dataset_name))\n",
    "if existing_datasets:\n",
    "    # read existing dataset\n",
    "    dataset = client.read_dataset(dataset_name=dataset_name)\n",
    "else:\n",
    "    dataset = client.create_dataset(dataset_name=dataset_name)\n",
    "    # Populate dataset\n",
    "    for _, row in df.iterrows():\n",
    "        q = row[\"Question\"]\n",
    "        a = row[\"Answer\"]\n",
    "        client.create_example(\n",
    "            inputs={\"question\": q}, outputs={\"answer\": a}, dataset_id=dataset.id\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Docugami KG-RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload files to Docugami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docugami import Docugami\n",
    "from docugami.lib.upload import upload_to_named_docset, wait_for_dgml\n",
    "\n",
    "dg_client = Docugami()\n",
    "file_paths = [FILES_DIR / file_name for file_name in FILE_NAMES]\n",
    "\n",
    "# Files will not be re-uploaded if they were previously uploaded (based on name)\n",
    "dg_docs = upload_to_named_docset(dg_client, file_paths, DOCSET_NAME)\n",
    "\n",
    "docset_id = \"\"\n",
    "docset_name = \"\"\n",
    "for doc in dg_docs:\n",
    "    if not docset_id:\n",
    "        docset_id = doc.docset.id\n",
    "    else:\n",
    "        # all docs must be in the same docset\n",
    "        assert docset_id == doc.docset.id\n",
    "\n",
    "    if not docset_name:\n",
    "        docset_name = dg_client.docsets.retrieve(doc.docset.id).name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'2022 Q3 AAPL.pdf': '/tmp/tmpwj7qvnpc',\n",
       " '2022 Q3 AMZN.pdf': '/tmp/tmp_b1v53gw',\n",
       " '2022 Q3 INTC.pdf': '/tmp/tmp7rbwg3oo',\n",
       " '2022 Q3 MSFT.pdf': '/tmp/tmplb2qcrzz',\n",
       " '2022 Q3 NVDA.pdf': '/tmp/tmpudk140xq',\n",
       " '2023 Q1 AAPL.pdf': '/tmp/tmpy49blnkv',\n",
       " '2023 Q1 AMZN.pdf': '/tmp/tmp9z7c6swf',\n",
       " '2023 Q1 INTC.pdf': '/tmp/tmpfi4rli61',\n",
       " '2023 Q1 MSFT.pdf': '/tmp/tmprbk948a0',\n",
       " '2023 Q1 NVDA.pdf': '/tmp/tmp779afiom',\n",
       " '2023 Q2 AAPL.pdf': '/tmp/tmpn22kjw46',\n",
       " '2023 Q2 AMZN.pdf': '/tmp/tmp3fadq9kp',\n",
       " '2023 Q2 INTC.pdf': '/tmp/tmpe0gim1ke',\n",
       " '2023 Q2 MSFT.pdf': '/tmp/tmpb5mb3x0a',\n",
       " '2023 Q2 NVDA.pdf': '/tmp/tmpgc20mcnv',\n",
       " '2023 Q3 AAPL.pdf': '/tmp/tmprnrsxhhs',\n",
       " '2023 Q3 AMZN.pdf': '/tmp/tmpuatxzleg',\n",
       " '2023 Q3 INTC.pdf': '/tmp/tmpgnyhizig',\n",
       " '2023 Q3 MSFT.pdf': '/tmp/tmp5dg44pcy',\n",
       " '2023 Q3 NVDA.pdf': '/tmp/tmp9dogliao'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Wait for files to finish processing (OCR, and zero-shot creation of XML knowledge graph)\n",
    "\n",
    "# Note: This can take some time on the free docugami tier (up to ~20 mins). Please contact us for faster paid plans.\n",
    "wait_for_dgml(dg_client, dg_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch not installed...\n",
      "Loading default rankgpt3 model for language en\n",
      "Loading RankGPTRanker model gpt-3.5-turbo\n",
      "Indexing SEC 10Q Filings (ID: s85dxu9aie2h)\n"
     ]
    }
   ],
   "source": [
    "# Run indexing\n",
    "from docugami_kg_rag.indexing import index_docset\n",
    "\n",
    "assert docset_id\n",
    "assert docset_name\n",
    "\n",
    "# Note: This can take some time since it is embedding and creating summaries for all the docs and chunks\n",
    "index_docset(docset_id=docset_id, name=docset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Docugami Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docugami_kg_rag.agent import agent as docugami_agent\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "def predict_docugami_agent(question: str) -> str:\n",
    "    return docugami_agent.invoke(\n",
    "        {\n",
    "            \"messages\": [HumanMessage(content=question)],\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the agent to make sure it is working\n",
    "predict_docugami_agent(\"How much did Microsoft spend for opex in the latest quarter?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up OpenAI Assistants Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create OpenAI Agent\n",
    "\n",
    "Please go to https://platform.openai.com/playground and create your agent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.openai_assistant import OpenAIAssistantRunnable\n",
    "\n",
    "def predict_openai_agent(input: dict, config: dict = None) -> dict:\n",
    "    openai_agent = OpenAIAssistantRunnable(assistant_id=OPENAI_ASSISTANT_ID, as_agent=True).with_config(config)\n",
    "    question = input[\"question\"]\n",
    "    result = openai_agent.invoke({\"content\": question})\n",
    "\n",
    "    return result.return_values[\"output\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the agent to make sure it is working\n",
    "predict_openai_agent({\"question\": \"How much did Microsoft spend for opex in the latest quarter?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Evals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from langsmith.client import Client\n",
    "from langchain.smith import RunEvalConfig\n",
    "from langchain.globals import set_llm_cache, get_llm_cache\n",
    "\n",
    "eval_config = RunEvalConfig(\n",
    "    evaluators=[\"qa\"],\n",
    ")\n",
    "\n",
    "\n",
    "def run_eval(eval_func, eval_run_name):\n",
    "    \"\"\"\n",
    "    Run eval\n",
    "    \"\"\"\n",
    "    client = Client()\n",
    "    client.run_on_dataset(\n",
    "        dataset_name=EVAL_NAME,\n",
    "        llm_or_chain_factory=eval_func,\n",
    "        evaluation=eval_config,\n",
    "        verbose=True,\n",
    "        project_name=eval_run_name,\n",
    "        concurrency_level=2,  # Reduced to help with rate limits, but will take longer\n",
    "    )\n",
    "\n",
    "\n",
    "# Experiments\n",
    "agent_map = {\n",
    "    \"docugami_kg_rag_zero_shot\": predict_docugami_agent,\n",
    "    \"openai_assistant_retrieval\": predict_openai_agent,\n",
    "}\n",
    "\n",
    "try:\n",
    "    # Disable global cache setting to get fresh results every time for all experiments\n",
    "    # since no caching or temperature-0 is supported for the openai assistants API and\n",
    "    # we want to measure under similar conditions\n",
    "    cache = get_llm_cache()\n",
    "    set_llm_cache(None)\n",
    "\n",
    "    for i in range(PER_EXPERIMENT_RUN_COUNT):\n",
    "        run_id = str(uuid.uuid4())\n",
    "        for project_name, agent in agent_map.items():\n",
    "            run_eval(agent, project_name + \"_\" + run_id)\n",
    "finally:\n",
    "    # Revert cache setting to global default\n",
    "    set_llm_cache(cache)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "app-sMPCFT4i-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
